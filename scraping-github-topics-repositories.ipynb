{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub\n",
    "\n",
    "TODO  (Intro): \n",
    "- Introduction about web scraping\n",
    "- Introduction about GitHub and the problem statement\n",
    "- Mention the tools you're using (Python, requests, Beautiful Soup, Pandas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Github\n",
    "\n",
    "Explain how you'll do it.\n",
    "\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_topic_page():\n",
    "    \n",
    "    topics_url=\"https://github.com/topics\"\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title=doc.find_all('p',{'class':selection_class}) #or p_tags=soup.find_all('p',class_selection_class)\n",
    "    topic_titles=[]\n",
    "    for titles in topic_title:\n",
    "        topic_titles.append(titles.text)\n",
    "    return topic_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_titles` can be used to get the list of titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(doc):\n",
    "    desc_selection=\"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_desc=doc.find_all('p',{'class':desc_selection})\n",
    "    topic_descriptions=[]\n",
    "    for desc in topic_desc:\n",
    "        topic_descriptions.append(desc.text.strip()) #.strip() is the method of string that remove spaces\n",
    "    return topic_descriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - example and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link=doc.find_all('a',{'class':'no-underline flex-grow-0'})\n",
    "\n",
    "    topic_urls=[]\n",
    "    base_url=\"https://github.com\"\n",
    "    for link in topic_link:\n",
    "        topic_urls.append(base_url+link['href'])\n",
    "    return topic_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url=\"https://github.com/topics\"\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topic_dict={\n",
    "        'title':get_topic_titles(doc),\n",
    "        'description':get_topic_desc(doc),\n",
    "        'url':get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topic_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top 25 repositories from a topic page\n",
    "\n",
    "TODO - explanation and step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "   #Downloadthe page\n",
    "    response=requests.get(topic_url)\n",
    "    \n",
    "    #check successfull response\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    #parse using beautyfulSoup\n",
    "    topic_doc=BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - talk about the h3 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tags,star_tag):\n",
    "    #returns all the required info about a respository\n",
    "    base_url=\"https://github.com\"\n",
    "    a_tags=h3_tags.find_all('a')\n",
    "    username=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url=base_url + a_tags[1]['href']\n",
    "    stars=parse_star_count(star_tag.text.strip())\n",
    "    return username,repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - show a example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "\n",
    "    #get the h3 tags containing repo title,repo url and username\n",
    "    h1_selection_class=\"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags=topic_doc.find_all('h3',{'class':h1_selection_class})\n",
    "   \n",
    "    #get star tags\n",
    "    star_tags = topic_doc.find_all('span',{'class':\"Counter js-social-count\"})\n",
    "    \n",
    "    #get repo info\n",
    "    topic_repo_dict={'username':[],\n",
    "                 'repo_name':[],\n",
    "                 'stars':[],\n",
    "                 'repo_url':[]\n",
    "                }\n",
    "    for i in range(len(repo_tags)):\n",
    "        #calling get_repo_info function\n",
    "        repo_info=get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repo_dict['username'].append(repo_info[0])\n",
    "        topic_repo_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repo_dict['stars'].append(repo_info[2])\n",
    "        topic_repo_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repo_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,path):\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping... \".format(path))\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - show an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str=stars_str.strip()\n",
    "    if stars_str[-1]=='k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    topic_df=scrape_topics()\n",
    "    \n",
    "    print('Scraping top topics from Github')\n",
    "    \n",
    "    os.makedirs('data',exist_ok = True)\n",
    "    for index,row in topic_df.iterrows():\n",
    "        print(row['title'],row['url'])\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top topics from Github\n",
      "3D https://github.com/topics/3d\n",
      "Scraping top repositories for \"3D\"\n",
      "Ajax https://github.com/topics/ajax\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Algorithm https://github.com/topics/algorithm\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Amp https://github.com/topics/amphp\n",
      "Scraping top repositories for \"Amp\"\n",
      "Android https://github.com/topics/android\n",
      "Scraping top repositories for \"Android\"\n",
      "Angular https://github.com/topics/angular\n",
      "Scraping top repositories for \"Angular\"\n",
      "Ansible https://github.com/topics/ansible\n",
      "Scraping top repositories for \"Ansible\"\n",
      "API https://github.com/topics/api\n",
      "Scraping top repositories for \"API\"\n",
      "Arduino https://github.com/topics/arduino\n",
      "Scraping top repositories for \"Arduino\"\n",
      "ASP.NET https://github.com/topics/aspnet\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Atom https://github.com/topics/atom\n",
      "Scraping top repositories for \"Atom\"\n",
      "Awesome Lists https://github.com/topics/awesome\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Amazon Web Services https://github.com/topics/aws\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Azure https://github.com/topics/azure\n",
      "Scraping top repositories for \"Azure\"\n",
      "Babel https://github.com/topics/babel\n",
      "Scraping top repositories for \"Babel\"\n",
      "Bash https://github.com/topics/bash\n",
      "Scraping top repositories for \"Bash\"\n",
      "Bitcoin https://github.com/topics/bitcoin\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Bootstrap https://github.com/topics/bootstrap\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "Bot https://github.com/topics/bot\n",
      "Scraping top repositories for \"Bot\"\n",
      "C https://github.com/topics/c\n",
      "Scraping top repositories for \"C\"\n",
      "Chrome https://github.com/topics/chrome\n",
      "Scraping top repositories for \"Chrome\"\n",
      "Chrome extension https://github.com/topics/chrome-extension\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "Command line interface https://github.com/topics/cli\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "Clojure https://github.com/topics/clojure\n",
      "Scraping top repositories for \"Clojure\"\n",
      "Code quality https://github.com/topics/code-quality\n",
      "Scraping top repositories for \"Code quality\"\n",
      "Code review https://github.com/topics/code-review\n",
      "Scraping top repositories for \"Code review\"\n",
      "Compiler https://github.com/topics/compiler\n",
      "Scraping top repositories for \"Compiler\"\n",
      "Continuous integration https://github.com/topics/continuous-integration\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "COVID-19 https://github.com/topics/covid-19\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "C++ https://github.com/topics/cpp\n",
      "Scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and display a CSV using Pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
